{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484160b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import timm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG_CONV:\n",
    "    model_path = '/kaggle/input/my-large-vit-path/conv_next_epoch2-3.pth'\n",
    "    model_name = 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384'  \n",
    "    input_size = 384\n",
    "    batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTestDatasetConv(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1316ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_conv(\n",
    "    images,\n",
    "    model_path,\n",
    "    model_name,\n",
    "    input_size,\n",
    "    batch_size\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    dataset = DiffusionTestDatasetConv(images, transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=384)\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tta_preds = None\n",
    "    for _ in range(2):\n",
    "        preds = []\n",
    "        for X in tqdm(dataloader, leave=False):\n",
    "            X = X.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X_out = model(X).cpu().numpy()\n",
    "                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)\n",
    "                X_out = normalize( X_out )\n",
    "                preds.append(X_out)\n",
    "                \n",
    "        if tta_preds is None:\n",
    "            tta_preds = np.vstack(preds).flatten()\n",
    "        else:\n",
    "            tta_preds += np.vstack(preds).flatten()\n",
    "    \n",
    "    return tta_preds / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135092d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n",
    "embeddings = predict_conv(images, CFG_CONV.model_path, CFG_CONV.model_name, CFG_CONV.input_size, CFG_CONV.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
