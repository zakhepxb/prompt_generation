{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import timm\n",
    "from timm.utils import AverageMeter\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel, AutoConfig\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7deba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/kaggle/input/my-dataset-of-conv/clip-vit-large-patch14_epoch2-3.pth'\n",
    "    model_config='/kaggle/input/my-clip-model/clip-vit-large-patch14-openai/config.json'\n",
    "    model_name = 'openai/clip-vit-large-patch14'\n",
    "    input_size = 224\n",
    "    batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTestDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Net, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(model_config, local_files_only=True)\n",
    "        clip = AutoModel.from_config(config)\n",
    "        self.vision = clip.vision_model\n",
    "        self.fc = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vision(x)['pooler_output']\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da003fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    images,\n",
    "    model_path,\n",
    "    model_config,\n",
    "    model_name,\n",
    "    input_size,\n",
    "    batch_size\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    dataset = DiffusionTestDataset(images, transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    model = Net(model_config)\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tta_preds = None\n",
    "    for _ in range(2):\n",
    "        preds = []\n",
    "        for X in tqdm(dataloader, leave=False):\n",
    "            X = X.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X_out = model(X).cpu().numpy()\n",
    "                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)\n",
    "                X_out = normalize( X_out )\n",
    "                preds.append(X_out)\n",
    "                \n",
    "        if tta_preds is None:\n",
    "            tta_preds = np.vstack(preds).flatten()\n",
    "        else:\n",
    "            tta_preds += np.vstack(preds).flatten()\n",
    "    \n",
    "    return tta_preds / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n",
    "embeddings = predict(images, CFG.model_path, CFG.model_config, CFG.model_name, CFG.input_size, CFG.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
